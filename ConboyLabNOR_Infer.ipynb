{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHDK7689Ei6A"
      },
      "source": [
        "## Install Ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hglwuAisELwG",
        "outputId": "3e3f4828-b71b-4975-930d-3cd5b8667604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.22 üöÄ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n",
            "Setup complete ‚úÖ (8 CPUs, 51.0 GB RAM, 29.9/201.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install line_profiler\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMoptwZ3ElpJ"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOMkECjdEnDR"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL\n",
        "import time\n",
        "import torch\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfqGnQ2LEhg8"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4QJlaSGEa0B",
        "outputId": "fd245629-511c-4173-8db9-5d7d7603d10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKn9SQFRE7ue"
      },
      "source": [
        "## Initialize YOLO Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43vqnOrfE9Fx",
        "outputId": "b930fd08-127a-4db7-819e-6eaeb5222939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ‚ö†Ô∏è /content/gdrive/MyDrive/ConboyLabCode/mouseBest.pt appears to require 'omegaconf', which is not in ultralytics requirements.\n",
            "AutoInstall will run now for 'omegaconf' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['omegaconf'] not found, attempting AutoUpdate...\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 79.5/79.5 kB 2.9 MB/s eta 0:00:00\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 117.0/117.0 kB 8.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e41f1dacafe83b5262a61207fa89270d10cf19e124e53f863254b5ac135aa2ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k2urun5a/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 6.7s, installed 1 package: ['omegaconf']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.set_device(0)\n",
        "localizerPath = '/content/gdrive/MyDrive/ConboyLabCode/objectLocalizerV3.pt'\n",
        "mousePath = '/content/gdrive/MyDrive/ConboyLabCode/mouseBest.pt'\n",
        "interactorPath = '/content/gdrive/MyDrive/ConboyLabCode/interactM.pt'\n",
        "yoloLocalizer = YOLO(localizerPath)\n",
        "yoloMouse = YOLO(mousePath)\n",
        "yoloInteractor = YOLO(interactorPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1oxRK1FbyV"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIJPvlpjuG4D"
      },
      "outputs": [],
      "source": [
        "def centroid(results):\n",
        "    # Assuming results.boxes.xyxy.cpu().numpy() returns a 2D array of shape (n_boxes, 4)\n",
        "    # where each row is [x1, y1, x2, y2] for a box.\n",
        "    coords = results.boxes.xyxy.cpu().numpy()\n",
        "\n",
        "    # Check if there are any boxes detected.\n",
        "    if coords.shape[0] == 0:\n",
        "        return None  # No boxes detected.\n",
        "\n",
        "    x1, y1, x2, y2 = coords[0]  # Access the first box coordinates.\n",
        "    centroid_obj = (int((x1 + x2) // 2), int((y1 + y2) // 2))\n",
        "    return centroid_obj\n",
        "\n",
        "# Approach Detection Functions\n",
        "def sortDist(l):\n",
        "  return l[0]\n",
        "\n",
        "def sortData(d):\n",
        "  return d[1]\n",
        "\n",
        "def sortByY(l):\n",
        "  return l[1]\n",
        "\n",
        "\n",
        "# Pipeline Function\n",
        "def calcDist(centroidM, centroidO):\n",
        "    distList = []\n",
        "    # Check Dist From Points\n",
        "    for l in range(len(centroidO)):\n",
        "      distList.append([math.dist(centroidM, centroidO[l][0]),centroidO[l][1]])\n",
        "      # Sort List\n",
        "      distList.sort(key=sortDist)\n",
        "    # Return Distance\n",
        "    return distList\n",
        "\n",
        "# Count Number of Approaches\n",
        "def count_groups_of_ones(lst, cons):\n",
        "    counter = 0  # Initialize the counter\n",
        "    i = 0  # Start iterating from the first index\n",
        "    while i < len(lst):\n",
        "        # Check if the current element is 0 and at least the next three elements contain a 1\n",
        "        if lst[i] == 0:\n",
        "            i += 1  # Move past the 0\n",
        "            start = i  # Mark the start of potential consecutive 1's\n",
        "            # Count the number of consecutive 1's following the 0\n",
        "            while i < len(lst) and lst[i] == 1:\n",
        "                i += 1\n",
        "            # If we found at least three consecutive 1's after a 0, increment the counter\n",
        "            if i - start >= cons:\n",
        "                counter += 1\n",
        "        else:\n",
        "            i += 1  # Move to the next element if the current one is not 0\n",
        "    return counter\n",
        "\n",
        "# Count per object Approach\n",
        "def count_obj_approach(lst):\n",
        "  # Bottom is index 0, top index 1\n",
        "  top = lst.count(1)\n",
        "  bottom = len(lst) - top\n",
        "  return [bottom, top]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNH8TwLJFPRQ"
      },
      "outputs": [],
      "source": [
        "def labelNOR(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer):\n",
        "  # create video capture object\n",
        "  cap = cv2.VideoCapture(video)\n",
        "  if not cap.isOpened():\n",
        "    exit()\n",
        "\n",
        "  # count the number of frames\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  totalNoFrames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "  durationInSeconds = totalNoFrames // fps\n",
        "\n",
        "  # Calculate Time Per Frame\n",
        "  frameT = durationInSeconds/totalNoFrames\n",
        "\n",
        "  # Establish Total Interact Counters (In Seconds)\n",
        "  leftTime = [0.0,0.0,0.0]\n",
        "  rightTime = [0.0,0.0,0.0]\n",
        "\n",
        "  # Seperate Into T1, T2, T3 (first 2:30 min, next 2:30 min, and last 5 min of each 10 min video)\n",
        "  T1, T2, T3 = 150, 300, 600\n",
        "  frameCount = 0\n",
        "\n",
        "  # Declare ARR\n",
        "  leftArr = []\n",
        "  rightArr = []\n",
        "  objArrL, objArrR = [] , []\n",
        "  objLeftInt, objRightInt = [] , []\n",
        "\n",
        "  # MouseCheck\n",
        "  firstMouse = False\n",
        "  firstObj = False\n",
        "  duration = 10 * 60  # 10 minutes in seconds\n",
        "  start_time = None\n",
        "\n",
        "  # Calculate the midpoint\n",
        "  midpoint = frame.shape[1] // 2\n",
        "\n",
        "  # Split the frame into left and right halves\n",
        "  left_frame = frame[:, :midpoint]\n",
        "  right_frame = frame[:, midpoint:]\n",
        "  # Read until video is completed\n",
        "  print(f\"------------------------------Video: {vidIndex} Started------------------------------\")\n",
        "\n",
        "  while cap.isOpened():\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    if frame is None:\n",
        "      break\n",
        "\n",
        "    # If frame is read correctly ret is True\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Only Run Pipeline over every sample rate one\n",
        "    if frameCount % sampleRate == 0:\n",
        "\n",
        "      # Progress Update\n",
        "      if frameCount % 1000 == 0:\n",
        "        print(f\"Processing: {frameCount}/{totalNoFrames} Completed {round((frameCount/(totalNoFrames))*100)}%\")\n",
        "\n",
        "      # Detect which Time Period its in\n",
        "      frametime = frameCount *frameT\n",
        "      framePeriod = \"T1\" if frametime < T2 else (\"T2\" if frametime < T3 else \"T3\")\n",
        "\n",
        "      # Check if mouse present\n",
        "      mouseCheck = yoloMouse([left_frame,right_frame], verbose = False, conf = 0.65, half=True)\n",
        "\n",
        "      # Run YOLO Interaction\n",
        "      if len(mouseCheck[0].boxes) > 0 or len(mouseCheck[1].boxes) > 0:\n",
        "\n",
        "        # Calculate Mouse Centroid\n",
        "        lC = centroid(mouseCheck[0])\n",
        "        rC = centroid(mouseCheck[1])\n",
        "\n",
        "        # Run Localizer\n",
        "        if firstObj == False:\n",
        "          objects = yoloLocalizer([left_frame,right_frame], verbose = False, conf = 0.4, half=True)\n",
        "          for i in range(len(objects[0].boxes)):\n",
        "            objArrL.append([centroid(objects[0][i])])\n",
        "          for j in range(len(objects[1].boxes)):\n",
        "            objArrR.append([centroid(objects[1][j])])\n",
        "          if objArrR == [] or objArrL == []:\n",
        "            continue\n",
        "          else:\n",
        "            # Sort Objects\n",
        "            sorted(objArrL , key=lambda k: [k[0][1], k[0][0]])\n",
        "            for l in range(len(objArrL)):\n",
        "              objArrL[l].append(l)\n",
        "            sorted(objArrR , key=lambda k: [k[0][1], k[0][0]])\n",
        "            for k in range(len(objArrR)):\n",
        "              objArrR[k].append(k)\n",
        "            firstMouse= True\n",
        "            firstObj = True\n",
        "\n",
        "        # Run Interaction Detector\n",
        "        results = yoloInteractor([left_frame,right_frame], verbose=False)\n",
        "\n",
        "        # Left\n",
        "        # Calculate Object Being Interacted with\n",
        "        if len(objArrL) > 1 and lC is not None:\n",
        "          objL = calcDist(lC,objArrL)\n",
        "          if objL[0][0] < objL[1][0]:\n",
        "            objLeftInt.append(objL[0][1])\n",
        "          else:\n",
        "            objLeftInt.append(objL[0][1])\n",
        "        if results[0].probs.top1 == 0:\n",
        "          leftArr.append(1)\n",
        "          if framePeriod == \"T1\":\n",
        "            leftTime[0] += frameT\n",
        "          elif framePeriod == \"T2\":\n",
        "            leftTime[1] += frameT\n",
        "          elif framePeriod == \"T3\":\n",
        "            leftTime[2] += frameT\n",
        "        else:\n",
        "          leftArr.append(0)\n",
        "\n",
        "        # Right\n",
        "        # Calculate Object Being Interacted with\n",
        "        if len(objArrR) > 1 and rC is not None:\n",
        "          objR = calcDist(rC,objArrR)\n",
        "          if objR[0][0] < objR[1][0]:\n",
        "            objRightInt.append(objR[0][1])\n",
        "          else:\n",
        "            objRightInt.append(objR[0][1])\n",
        "        if results[1].probs.top1 == 0:\n",
        "          rightArr.append(1)\n",
        "          if framePeriod == \"T1\":\n",
        "            rightTime[0] += frameT\n",
        "          elif framePeriod == \"T2\":\n",
        "            rightTime[1] += frameT\n",
        "          elif framePeriod == \"T3\":\n",
        "            rightTime[2] += frameT\n",
        "        else:\n",
        "          rightArr.append(0)\n",
        "\n",
        "    # Update Frame Count\n",
        "    frameCount += 1\n",
        "\n",
        "    # Stop if 10 minutes are processed\n",
        "    if frameCount * frameT >= 10 * 60:\n",
        "        break\n",
        "\n",
        "  # Run Approach Detection\n",
        "  rightAp = count_groups_of_ones(rightArr,3)\n",
        "  leftAp = count_groups_of_ones(leftArr,3)\n",
        "\n",
        "  # Run Per Object Count\n",
        "  rightObj = count_obj_approach(objRightInt)\n",
        "  leftObj = count_obj_approach(objLeftInt)\n",
        "\n",
        "  # Calculate Average Time Per Approach\n",
        "  rightApT = rightAp/frameT\n",
        "  leftApT = leftAp/frameT\n",
        "\n",
        "  return durationInSeconds, leftTime, rightTime, rightAp, leftAp, rightApT, leftApT, rightObj, leftObj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO31dv1I57LL"
      },
      "source": [
        "## Run Video Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX6UgOnhVQnC"
      },
      "outputs": [],
      "source": [
        "folder = '/content/gdrive/MyDrive/NOR COPY + New NOR/*2022/2022 Part 1 (22 Videos)/'\n",
        "video_extensions = ['.mp4', '.avi', '.MOV', '.mkv']\n",
        "column = [\"VideoName\", \"duration\", \"leftTime\", \"rightTime\", \"rightAp\", \"leftAp\", \"rightApT\", \"leftApT\", \"rightObj\", \"leftObj\"]\n",
        "dfTot = pd.DataFrame(columns=column)\n",
        "vidIndex = 1\n",
        "sampleRate = 3\n",
        "for monthFolder in os.listdir(folder):\n",
        "  path = os.path.join(folder, monthFolder)\n",
        "  for file in os.listdir(path):\n",
        "    if any(file.endswith(ext) for ext in video_extensions):\n",
        "      video = os.path.join(path, file)\n",
        "      totalDuration, leftTime, rightTime, rightAp, leftAp, rightApT, leftApT, rightObj, leftObj = labelNOR(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer)\n",
        "      data = {\n",
        "        \"VideoName\": [video],\n",
        "        \"duration\": [totalDuration],\n",
        "        \"leftTime\": [leftTime],\n",
        "        \"rightTime\": [rightTime],\n",
        "        \"rightAp\": [rightAp],\n",
        "        \"leftAp\": [leftAp],\n",
        "        \"rightApT\": [rightApT],\n",
        "        \"leftApT\": [leftApT],\n",
        "        \"rightObj\": [rightObj],\n",
        "        \"leftObj\": [leftObj],\n",
        "      }\n",
        "      df = pd.DataFrame(data)\n",
        "      dfTot = pd.concat([dfTot, df], ignore_index=True)\n",
        "      vidIndex += 1\n",
        "current_datetime = datetime.now()\n",
        "dfTot.to_csv(os.path.join(folder, f\"{current_datetime}.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh0qYbDt54mU"
      },
      "source": [
        "## Individual Video Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "TdwQK6OtGek4",
        "outputId": "f955b450-01f5-493e-e1db-1d204bd77fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The line_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext line_profiler\n",
            "------------------------------Video: 1 Started------------------------------\n",
            "Processing: 0/11579.0 Completed 0%\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([], device='cuda:0')\n",
            "conf: tensor([], device='cuda:0')\n",
            "data: tensor([], device='cuda:0', size=(0, 6))\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (1920, 540)\n",
            "shape: torch.Size([0, 6])\n",
            "xywh: tensor([], device='cuda:0', size=(0, 4))\n",
            "xywhn: tensor([], device='cuda:0', size=(0, 4))\n",
            "xyxy: tensor([], device='cuda:0', size=(0, 4))\n",
            "xyxyn: tensor([], device='cuda:0', size=(0, 4))\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.], device='cuda:0')\n",
            "conf: tensor([0.9600], device='cuda:0')\n",
            "data: tensor([[1.4288e+02, 9.2175e+02, 3.5925e+02, 9.9375e+02, 9.5996e-01, 0.0000e+00]], device='cuda:0')\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (1920, 540)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[251.0625, 957.7500, 216.3750,  72.0000]], device='cuda:0')\n",
            "xywhn: tensor([[0.4649, 0.4988, 0.4007, 0.0375]], device='cuda:0')\n",
            "xyxy: tensor([[142.8750, 921.7500, 359.2500, 993.7500]], device='cuda:0')\n",
            "xyxyn: tensor([[0.2646, 0.4801, 0.6653, 0.5176]], device='cuda:0')\n",
            "Processing: 3000/11579.0 Completed 26%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-72b26c9dd067>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#%lprun -f labelNOR labelNOR(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtotalDuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightAp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftAp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightApT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftApT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelNOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoloInteractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoloMouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoloLocalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-8c480c54b2b3>\u001b[0m in \u001b[0;36mlabelNOR\u001b[0;34m(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Run Interaction Detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myoloInteractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencapsulated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mResults\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     def track(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;31m# Preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim0s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/classify/predict.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 img = torch.stack(\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 )\n\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/classify/predict.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 img = torch.stack(\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 )\n\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 )\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%load_ext line_profiler\n",
        "video = '/content/gdrive/MyDrive/NOR COPY + New NOR/*2021/2021 Part 1 A (22 Videos)/1.2.21 NOR COPY/1.2.21-18-14-Encoding.MOV'\n",
        "sampleRate = 3\n",
        "vidIndex = 1\n",
        "#%lprun -f labelNOR labelNOR(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer)\n",
        "\n",
        "totalDuration, leftTime, rightTime, rightAp, leftAp, rightApT, leftApT, rightObj, leftObj = labelNOR(vidIndex, video, sampleRate, yoloInteractor, yoloMouse, yoloLocalizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8RwlhNdbBM2"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Duration: {totalDuration}\")\n",
        "print(f\"Left Time: {leftTime}\")\n",
        "print(f\"Right Time: {rightTime}\")\n",
        "print(f\"Right Approach: {rightAp}\")\n",
        "print(f\"Left Approach: {leftAp}\")\n",
        "print(f\"Right Approach Time: {rightApT}\")\n",
        "print(f\"Left Approach Time: {leftApT}\")\n",
        "print(f\"Right Objects: {rightObj}\")\n",
        "print(f\"Left Objects: {leftObj}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QaM0nB79K2I"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "  \"VideoName\": [video],\n",
        "  \"duration\": [totalDuration],\n",
        "  \"leftTime\": [leftTime],\n",
        "  \"rightTime\": [rightTime],\n",
        "  \"rightAp\": [rightAp],\n",
        "  \"leftAp\": [leftAp],\n",
        "  \"rightApT\": [rightApT],\n",
        "  \"leftApT\": [leftApT],\n",
        "  \"rightObj\": [rightObj],\n",
        "  \"leftObj\": [leftObj],\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(f\"{video}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5xUDWqh5ET5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}